{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the components that Airflow provides. Please note: Airflow DAGs are to be specified in `.py` files, not Jupyter notebooks. This notebook only serves the presentation purpose and can not be directly used in Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Acyclic Graph (DAG)\n",
    "\n",
    "Each Airflow pipeline has to be an directed acyclic graph (i.e. no loops, clear task dependencies).\n",
    "We define a DAG in the following way:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "\n",
    "# the default args will be passed to all tasks belonging to the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2019, 8, 1),\n",
    "    ...\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'keras2production-pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    ...\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "DAGs need to be placed into the folder `$AIRFLOW_HOME/dags`, where `$AIRFLOW_HOME` is usually set to `/usr/local/airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators\n",
    "\n",
    "Each pipeline consists of steps, which are called operators. Operators execute some kind of task and finish when the task is done. There are also sensors, a special type of operators, which finish when a condition is met (i.e. another task has finished or a file exists). Don't worry so much about the difference between a normal operator and a sensor, just lookup what fits your needs the best in the well-documented API.\n",
    "\n",
    "This might be the most used operator: The `PythonOperator`.\n",
    "\n",
    "```python\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def print_sth():\n",
    "    print(\"Our slides are still there!\")\n",
    "    \n",
    "python_task = PythonOperator(\n",
    "    task_id=\"print\",\n",
    "    python_callable=print_sth,\n",
    "    provide_context=False,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "This is an example of a sensor:\n",
    "\n",
    "```python\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "fs_task = FileSensor(\n",
    "    task_id=\"lookup_file\",\n",
    "    filepath=\"/keras2production/slides/slides.pdf\",\n",
    "    dag=dag\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Operators\n",
    "\n",
    "Airflow uses the bitshift operators `>>` and `<<` to connect pipeline steps together (you can also use `set_upstream()` or `set_downstream()`, but just don't...).\n",
    "\n",
    "`task1 >> task2` means: `task1` will be executed before `task2`, and vice versa for `<<`.\n",
    "\n",
    "**Tip**: Only use the `>>` to keep your code consistent and readable.\n",
    "\n",
    "Let's connect our operator and sensor that we defined above:\n",
    "\n",
    "```python\n",
    "fs_task >> python_task\n",
    "```\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCom\n",
    "\n",
    "Ideally all of our operators and sensors are independent of each other, that is: No variables, states etc. should have to be communicated between them. This allows an easy exchange of tasks. Since we cannot always avoid communication between tasks, Airflow provides the XCom (_cross-communication_) system, which follows a simple push-pull idea.\n",
    "\n",
    "You can push a variable in an operator via `xcom_push()` or the `return` statement, and pull (i.e. get) the variable in an other task by `xcom_pull()`.\n",
    "\n",
    "This is an example, how two python operators could communicate with each other via XCom:\n",
    "\n",
    "```python\n",
    "def foo():\n",
    "    x = \"hello world\"\n",
    "    return x\n",
    "\n",
    "\n",
    "def bar(**context):\n",
    "    x = context[\"task_instance\"].xcom_pull(task_ids=\"task1\")\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "task1 = PythonOperator(\n",
    "    task_id=\"task1\",\n",
    "    python_callable=foo,\n",
    ")\n",
    "\n",
    "task2 = PythonOperator(\n",
    "    task_id=\"task2\",\n",
    "    python_callable=bar,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "task1 >> task2\n",
    "```\n",
    "\n",
    "**Note**: The pulling task needs to be provided with context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The web interface\n",
    "\n",
    "To monitor and manage your pipelines, Airflow offers a web interface. You can find it today at `localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is so much more!\n",
    "\n",
    "Airflow has many concepts to offer, which allow a robust but flexible execution. Some highlights are:\n",
    "- Error and retry management\n",
    "- Different executors (Celery, Sequential, Local), allowing for parallel execution on multiple nodes\n",
    "- Integration with GCP, AWS, Azure, ...\n",
    "\n",
    "Check out the [documentation page](https://airflow.apache.org/) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
